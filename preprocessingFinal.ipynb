{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting Output\n",
    "import pandas as pd\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\QueryResults (1).csv')\n",
    "csv_input['Output'] = (csv_input['qAcceptedAnswerId']==csv_input['aId'])\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising ascore\n",
    "import pandas as pd\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "dist = csv_input.groupby('aParentId')['aScore'].transform('max')\n",
    "dist1=csv_input.groupby('aParentId')['aScore'].transform('min')\n",
    "dist= dist-dist1\n",
    "#csv_input.loc[:, csv_input.drop('aScore', axis=1).columns] = (dist.sub(csv_input.drop('aScore', axis=1)).div(dist))\n",
    "csv_input['anewScore']=(csv_input['aScore']-dist1).div(dist)\n",
    "csv_input['anewScore'] = csv_input['anewScore'].fillna(0)\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising Commentcount\n",
    "import pandas as pd\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "dist =  csv_input.groupby('aParentId')['aCommentCount'].transform('max')\n",
    "dist1=csv_input.groupby('aParentId')['aCommentCount'].transform('min')\n",
    "dist= dist-dist1\n",
    "#csv_input.loc[:, csv_input.drop('aScore', axis=1).columns] = (dist.sub(csv_input.drop('aScore', axis=1)).div(dist))\n",
    "csv_input['anewCommentCount']=(csv_input['aCommentCount']-dist1).div(dist)\n",
    "csv_input['anewCommentCount'] = csv_input['anewCommentCount'].fillna(0)\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting order of the answer\n",
    "import pandas as pd\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "dist =  csv_input.groupby('aParentId')['aCreationDate'].cumcount()+1\n",
    "csv_input['aorder']=dist\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tokenize\n",
    "import io\n",
    "import nltk \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "     \n",
    "def get_cosine_sim(strs):\n",
    "    vectors = [t for t in get_vectors(strs)]\n",
    "    k=cosine_similarity(vectors)\n",
    "    return k[0][1]\n",
    "     \n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag): \n",
    "    if nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    elif nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence): \n",
    "    tokens=[w for w in nltk.word_tokenize(sentence)]\n",
    "    word=[]\n",
    "    for i in tokens:\n",
    "        if i not in stop_words:\n",
    "            word.append(i)\n",
    "    pos_tagged = nltk.pos_tag(word) \n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged)) \n",
    "    result = [] \n",
    "    for word, tag in wordnet_tagged: \n",
    "        if tag is None: \n",
    "            continue\n",
    "        else: \n",
    "            result.append(lemmatizer.lemmatize(word, tag)) \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_vectors(strs):\n",
    "    text = [t for t in strs]\n",
    "    try:\n",
    "        vectorizer = CountVectorizer(tokenizer=lemmatize_sentence)\n",
    "        k=vectorizer.fit_transform(text).toarray()\n",
    "    except:\n",
    "         return [[1,1],[1,1]]\n",
    "    return k\n",
    " \n",
    "def get_code_cosine_sim(strs):\n",
    "    temp=strs[:]\n",
    "    n=len(strs)\n",
    "    #55 - comment\n",
    "    stops=[55,4,0]\n",
    "    d=dict()\n",
    "    k=[]\n",
    "    try:\n",
    "        for i in temp:\n",
    "             for token in tokenize.generate_tokens(io.StringIO(i).readline):\n",
    "                if token.type not in stops and not(token.type== 3 and (token.string.startswith('\"\"\"') or token.string.startswith(\"'''\"))):\n",
    "                    space=token.string.replace(' ','')\n",
    "                    if(len(space)==0):\n",
    "                        continue\n",
    "                    index=token.string.find('\\n')\n",
    "                    index1=token.string.find('\\r')\n",
    "                    if(index==-1 and index1==-1):\n",
    "                        k.append(token.string)\n",
    "                    elif(index>0 and index1==-1):\n",
    "                        if(token.string[index-1]=='\\''):\n",
    "                            k.append(token.string)\n",
    "                    elif(index1==-1 and index1>0):\n",
    "                        if(token.string[index1-1]=='\\''):\n",
    "                            k.append(token.string)\n",
    "                    elif(index1>0 and index>0):\n",
    "                        k.append(token.string)\n",
    "                        \n",
    "        for j in k:\n",
    "            d[j]=[0]*n\n",
    "        count=0\n",
    "        for i in temp:\n",
    "            for token in tokenize.generate_tokens(io.StringIO(i).readline):\n",
    "                if token.string in k:\n",
    "                    d[token.string][count]+=1\n",
    "            count+=1\n",
    "        a=[]\n",
    "        b=[]\n",
    "        for key, val in d.items():\n",
    "                 a.append(val[0])\n",
    "                 b.append(val[1])\n",
    "        vectors=[]\n",
    "        vectors.append(a)\n",
    "        vectors.append(b)\n",
    "        try:\n",
    "            k=cosine_similarity(vectors)\n",
    "            return k[0][1]\n",
    "        except:\n",
    "            #no code segment\n",
    "            return 1\n",
    "    except:\n",
    "        vectorizer = CountVectorizer(tokenizer=word_tokenize)\n",
    "        vectors=vectorizer.fit_transform(temp).toarray()\n",
    "        k=cosine_similarity(vectors)\n",
    "        return k[0][1]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l=[]\n",
    "for i,j in zip(csv_input['qBody'], csv_input['aBody']):\n",
    "    soup = BeautifulSoup(i,features=\"lxml\")\n",
    "    ans=\"\"\n",
    "    ans1=\"\"\n",
    "    soup1 = BeautifulSoup(j,features=\"lxml\")\n",
    "    que=\"\"\n",
    "    que1=\"\"\n",
    "    qsim=0\n",
    "    csim=0\n",
    "\n",
    "    c_qtags=soup.findAll('pre')\n",
    "    remove1=soup.findAll('a')\n",
    "    c_atags=soup1.findAll('pre')\n",
    "    remove2=soup1.findAll('a')\n",
    "    for node in c_qtags:\n",
    "        que1+=node.get_text()\n",
    "        node.extract()\n",
    "    for node in remove1:\n",
    "        node.extract()\n",
    "    que=soup.get_text()\n",
    "    for node in c_atags:\n",
    "        ans1+=node.get_text()\n",
    "        node.extract()\n",
    "    for node in remove2:\n",
    "        node.extract()\n",
    "    ans=soup1.get_text()\n",
    "    qsim=get_cosine_sim([que,ans])\n",
    "    csim=get_code_cosine_sim([que1,ans1])\n",
    "    l.append([qsim,csim ])\n",
    "\n",
    "df = pd.DataFrame(l, columns = ['QAsim','QAcodesim'])\n",
    "csv_input['QAsim']=df['QAsim']\n",
    "csv_input['QAcodesim ']=df['QAcodesim']\n",
    "\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "dist =  csv_input.groupby('aParentId')\n",
    "simt=[]\n",
    "simc=[]\n",
    "f=0\n",
    "for i in dist.aBody.unique():\n",
    "    text=[]\n",
    "    code=[]\n",
    "    anst=\"\"\n",
    "    ansc=\"\"\n",
    "    for j in i:\n",
    "        f+=1\n",
    "        j=str(j)\n",
    "        soup = BeautifulSoup(j,\"lxml\")\n",
    "        to_remove = soup.find_all(\"pre\") \n",
    "        to_remove1=soup.find_all(\"a\")\n",
    "        for node in to_remove:\n",
    "            ansc+=node.get_text()\n",
    "            node.extract()\n",
    "        for node in to_remove1:\n",
    "            node.extract()\n",
    "        anst=soup.get_text()\n",
    "        text.append(anst)\n",
    "        code.append(ansc)\n",
    "    print(ansc)\n",
    "    print(anst)\n",
    "    for j in range(0,len(code)):\n",
    "        sum1=0\n",
    "        sum2=0\n",
    "        for k in range(0,len(code)):\n",
    "            if(j!=k and (text[j]!='' or text[k]!='')):\n",
    "                sum1+=get_cosine_sim([text[j],text[k]])\n",
    "            elif j!=k:\n",
    "                sum1+=1\n",
    "            if(j!=k and (code[j]!=\"\" or code[k]!=\"\")):\n",
    "                sum2+=get_code_cosine_sim([code[j],code[k]])\n",
    "            elif j!=k:\n",
    "                sum2+=1\n",
    "        simt.append(sum1/(len(code)-1))\n",
    "        simc.append(sum2/(len(code)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(simt, columns = ['AAsim'])\n",
    "csv_input['AAsim']=df['AAsim']\n",
    "df = pd.DataFrame(simc, columns = ['AAcodesim'])\n",
    "csv_input['AAcodesim']=df['AAcodesim']\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "dist =  csv_input.groupby('aParentId')['QAsim'].transform('max')\n",
    "dist2 =  csv_input.groupby('aParentId')['QAsim'].transform('min')\n",
    "dist1=  csv_input.groupby('aParentId')['QAcodesim '].transform('max')\n",
    "dist3=  csv_input.groupby('aParentId')['QAcodesim '].transform('min')\n",
    "dist=dist-dist2\n",
    "dist1=dist1-dist3\n",
    "csv_input['QAnorm']=(csv_input['QAsim']-dist2).div(dist)\n",
    "csv_input['QAnorm'] = csv_input['QAnorm'].fillna(0)\n",
    "csv_input['QAcnorm']=(csv_input['QAcodesim ']-dist3).div(dist1)\n",
    "csv_input['QAcnorm'] = csv_input['QAcnorm'].fillna(0)\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_input = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "dist =  csv_input.groupby('aParentId')['AAsim'].transform('max')\n",
    "dist2 =  csv_input.groupby('aParentId')['AAsim'].transform('min')\n",
    "dist1=  csv_input.groupby('aParentId')['AAcodesim'].transform('max')\n",
    "dist3=  csv_input.groupby('aParentId')['AAcodesim'].transform('min')\n",
    "dist=dist-dist2\n",
    "dist1=dist1-dist3\n",
    "csv_input['AAnorm']=(csv_input['AAsim']-dist2).div(dist)\n",
    "csv_input['AAnorm'] = csv_input['AAnorm'].fillna(0)\n",
    "csv_input['AAcnorm']=(csv_input['AAcodesim']-dist3).div(dist1)\n",
    "csv_input['AAcnorm'] = csv_input['AAcnorm'].fillna(0)\n",
    "csv_input.to_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\admin\\Downloads\\datasettemp.csv')\n",
    "x1 = df.loc[df['Output']==0]\n",
    "x2 = df.loc[df['Output']==1]\n",
    "x1=x1.sample(frac=1)\n",
    "X_false=x1.iloc[:4013,:]\n",
    "X_true=x2.iloc[:,:]\n",
    "X = pd.concat([X_false, X_true])\n",
    "X.to_csv(r'C:\\Users\\admin\\Desktop\\fypnew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
